<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2 Final//EN">

<!--Converted with LaTeX2HTML 2002-2-1 (1.71)
original version by:  Nikos Drakos, CBLU, University of Leeds
* revised and updated by:  Marcus Hennecke, Ross Moore, Herb Swan
* with significant contributions from:
  Jens Lippmann, Marek Rouchal, Martin Wilck and others -->
<HTML>
<HEAD>
<TITLE>RL-Glue 3.0</TITLE>
<META NAME="description" CONTENT="RL-Glue 3.0">
<META NAME="keywords" CONTENT="Glue-Overview">
<META NAME="resource-type" CONTENT="document">
<META NAME="distribution" CONTENT="global">

<META NAME="Generator" CONTENT="LaTeX2HTML v2002-2-1">
<META HTTP-EQUIV="Content-Style-Type" CONTENT="text/css">

<LINK REL="STYLESHEET" HREF="Glue-Overview.css">

</HEAD>

<BODY >
<!--Navigation Panel-->
<IMG WIDTH="81" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next_inactive" SRC="nx_grp_g.png"> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev_g.png">   
<BR>
<BR>
<BR>
<!--End of Navigation Panel-->
<H1 ALIGN="CENTER">RL-Glue 3.0 Overview</H1>
<DIV>

<P ALIGN="CENTER"><STRONG>Adam White</STRONG></P>
</DIV>
<BR>

<H2><A NAME="SECTION00010000000000000000">
Contents</A>
</H2>
<!--Table of Contents-->

<UL>
<LI><A NAME="tex2html62"
  HREF="Glue-Overview.html#SECTION00020000000000000000">Introduction</A>
<UL>
<LI><A NAME="tex2html63"
  HREF="Glue-Overview.html#SECTION00021000000000000000">Purpose of Document</A>
<LI><A NAME="tex2html64"
  HREF="Glue-Overview.html#SECTION00022000000000000000">How to Use This Document</A>
</UL>
<BR>
<LI><A NAME="tex2html65"
  HREF="Glue-Overview.html#SECTION00030000000000000000">RL-Glue Concepts</A>
<UL>
<LI><A NAME="tex2html66"
  HREF="Glue-Overview.html#SECTION00031000000000000000">Agents, Environments and Experiment Programs</A>
</UL>
<BR>
<LI><A NAME="tex2html67"
  HREF="Glue-Overview.html#SECTION00040000000000000000">RL-Glue Environment Programs</A>
<UL>
<LI><A NAME="tex2html68"
  HREF="Glue-Overview.html#SECTION00041000000000000000">Essential Components Of A RL-Glue Environment</A>
<LI><A NAME="tex2html69"
  HREF="Glue-Overview.html#SECTION00042000000000000000">Additional Components Of A RL-Glue Environment</A>
</UL>
<BR>
<LI><A NAME="tex2html70"
  HREF="Glue-Overview.html#SECTION00050000000000000000">RL-Glue Agent Programs</A>
<UL>
<LI><A NAME="tex2html71"
  HREF="Glue-Overview.html#SECTION00051000000000000000">Essential Components Of A RL-Glue Agent</A>
<LI><A NAME="tex2html72"
  HREF="Glue-Overview.html#SECTION00052000000000000000">Additional Components Of A RL-Glue Agent</A>
</UL>
<BR>
<LI><A NAME="tex2html73"
  HREF="Glue-Overview.html#SECTION00060000000000000000">RL-Glue Experiment Programs</A>
<UL>
<LI><A NAME="tex2html74"
  HREF="Glue-Overview.html#SECTION00061000000000000000">Basic Experiment Programs</A>
<LI><A NAME="tex2html75"
  HREF="Glue-Overview.html#SECTION00062000000000000000">Advanced Experiment Programs</A>
</UL>
<BR>
<LI><A NAME="tex2html76"
  HREF="Glue-Overview.html#SECTION00070000000000000000">Command and Function Reference</A>
<UL>
<LI><A NAME="tex2html77"
  HREF="Glue-Overview.html#SECTION00071000000000000000">Agent Functions</A>
<LI><A NAME="tex2html78"
  HREF="Glue-Overview.html#SECTION00072000000000000000">Environment Functions</A>
<LI><A NAME="tex2html79"
  HREF="Glue-Overview.html#SECTION00073000000000000000">Interface Routines Provided by the RL-Glue</A>
</UL>
<BR>
<LI><A NAME="tex2html80"
  HREF="Glue-Overview.html#SECTION00080000000000000000">Changes from RL-Glue 2.x</A>
<UL>
<LI><A NAME="tex2html81"
  HREF="Glue-Overview.html#SECTION00081000000000000000">The Codec Split</A>
<LI><A NAME="tex2html82"
  HREF="Glue-Overview.html#SECTION00082000000000000000">Build Changes</A>
<LI><A NAME="tex2html83"
  HREF="Glue-Overview.html#SECTION00083000000000000000">API Changes</A>
<LI><A NAME="tex2html84"
  HREF="Glue-Overview.html#SECTION00084000000000000000">Type Changes</A>
</UL>
<BR>
<LI><A NAME="tex2html85"
  HREF="Glue-Overview.html#SECTION00090000000000000000">Frequently Asked Questions</A>
<UL>
<LI><A NAME="tex2html86"
  HREF="Glue-Overview.html#SECTION00091000000000000000">Where did the task spec parsers go?</A>
<LI><A NAME="tex2html87"
  HREF="Glue-Overview.html#SECTION00092000000000000000">Can I write my agents in &lt; insert language here &gt;</A>
<LI><A NAME="tex2html88"
  HREF="Glue-Overview.html#SECTION00093000000000000000">Does Rl-Glue support multi-agent reinforcement learning?</A>
<LI><A NAME="tex2html89"
  HREF="Glue-Overview.html#SECTION00094000000000000000">Why isn't the RL-Glue interface object oriented?</A>
</UL>
<BR>
<LI><A NAME="tex2html90"
  HREF="Glue-Overview.html#SECTION000100000000000000000">Authors</A>
</UL>
<!--End of Table of Contents-->
<P>

<H1><A NAME="SECTION00020000000000000000">
Introduction</A>
</H1>

<H2><A NAME="SECTION00021000000000000000">
Purpose of Document</A>
</H2>
This document has been laid out so that it can be used for: 

<OL>
<LI><B>RL-Glue what?</B> learning about RL-Glue at an abstract level
</LI>
<LI><B>Compatibility:</B> making existing C/C++ agents and environments work with RL-Glue
</LI>
<LI><B>Plugging agents and environments together:</B> how to write an experiment programs
</LI>
<LI><B>What function do I use to:</B> quick function reference for RL-Glue
</LI>
</OL>

<P>
Recently (September 08) the RL-Glue Project has been split into the RL-Glue Project and RL-Glue Extensions Project. The RL-Glue Project now only includes the RL-Glue interface and plugs for C/C++ agents, environments and experiment programs. The RL-Glue Extensions Project contains codecs for Java, Python and Matlab. The Extensions Project contains the multi-language support code that used to be part of RL-Glue. This document contains <B>NO</B> technical details for writing programs: everything is abstracted. See the RL-Glue Technical Documentation distributed with RL-Glue and codec documentation for language specific details on how to implement agents, environments and experiment programs in C/C++, Java, Python and Matlab.

<P>

<H2><A NAME="SECTION00022000000000000000">
How to Use This Document</A>
</H2>

<P>
This document as been subdivided to reflect the five purposed described above. To learn about the major components of RL-Glue and a description of how those components interact see Section <A HREF="#RL-Glue">2</A>. To learn how to make an environment and agent programs compatible with RL-Glue we recommend sections <A HREF="#envp1">3.1</A> and <A HREF="#agentp1">4.1</A>. Sections <A HREF="#envp1">3.1</A> and <A HREF="#agentp1">4.1</A> describe only the mandatory functions that RL-Glue environments and agents must implement. Sections <A HREF="#envp2">3.2</A> and <A HREF="#agentp2">4.2</A> describe optional environment and agent functions. To learn about experiment programs and how they interact with RL-Glue see Section <A HREF="#exp">5</A>. For quick function reference see Section <A HREF="#ref">6</A>.

<P>
Frequently asked questions can be found in Section <A HREF="#faq">8</A>. A summary of and explanations for all changes from RL-Glue 2.X to Rl-Glue 3.0 can be found in Section <A HREF="#change">7</A>.

<P>
This document and the RL-Glue Project using naming conventions and definitions from Sutton and Barto's ``Reinforcement Learning: An Introduction". We recommend that you review Sutton and Barto to get the most out of the RL-Glue documentation. An online version of Sutton and Barto can be founf here: <TT><A NAME="tex2html1"
  HREF="http://www.cs.ualberta.ca/~sutton/book/the-book.html">http://www.cs.ualberta.ca/~sutton/book/the-book.html</A></TT>.

<P>

<H1><A NAME="SECTION00030000000000000000"></A>
<A NAME="RL-Glue"></A>
<BR>
RL-Glue Concepts
</H1>
A large part of studying and researching reinforcement learning is experimentation. When writing an agent, ensuring it makes exploratory moves to discover the world is important to achieving an optimal policy. It is similarly important that experimenters are easily able to "explore" new algorithms and ideas without the prohibitive cost of writing the necesary experimentation code. One of the functions of RL-Glue is to simplify and speed up the process of writing an experiment so that every idea can be tested. 

<P>
Another important part of learning is evaluation and improvement. With agents, learning is often a cycle of policy and representation evaluation and improvement.  An agent will roam the world and gain more information about the states it encounters. The agent then uses this new data to evaluate how accurate the value function was and adjusts its behaviour (policy) based on the new value function. Similiarily, in research and development it is important to look at other work being done in the field, compare your own performance and then improve. One goal for RL-Glue is to provide a consistent tool for running and comparing varied agents and environments from diverse sources. A common problem for reinforcement learning researchers arrises when an experimenter wishes to compare their own work with previously established results. Pre-RL-Glue, the solution was often to reverse engineer code for the experiment based on the results and environment/agent descriptions provided in papers.  When an author provided their own source, there was still the issue of deciphering the original code and piecing in a new agent or environment. With RL-Glue, an author can make the necessary RL-Glue agent/environment/experiment programs  available to the public such that another author can rerun the original experiment and easily plug in their own code to compare performance.  Competitions for agents using RL-Glue have been run at NIPS and ICML in recent years, further exemplifying the utility of RL-Glue to the research community.

<P>

<DIV ALIGN="CENTER"><A NAME="fig1"></A><A NAME="44"></A>
<TABLE>
<CAPTION ALIGN="BOTTOM"><STRONG>Figure 1:</STRONG>
The RL-Glue Standard. Arrows indicate function call direction.</CAPTION>
<TR><TD>
<DIV ALIGN="CENTER">
<IMG
  WIDTH="427" HEIGHT="237" ALIGN="BOTTOM" BORDER="0"
 SRC="./glue_connections_no_shadow.png"
 ALT="Image glue_connections_no_shadow">

</DIV></TD></TR>
</TABLE>
</DIV>

<P>
RL-Glue is both a set of ideas and standards, as well as a software implementation. In theory, RL-Glue is a protocol for all RL researchers to follow. Having this very simple standard of necessary functions facilitates the exchange and comparison of agents and environments without limiting their abilities. As software, RL-Glue is functionally a test harness to ``plug in'' agents, environments and experiment programs (previously titled benchmarks) without having to continually rewrite the connecting code for these pieces. An experiment program is some very simple code stating how many times to run an agent in an environment and what data should be extracted from the agent and environment. Provided the agent, environment, and experiment program follow the RL-Glue protocol by implementing the few necessary functions, they can easily be plugged in with the RL-Glue code to have an experiment running quite effortlessly. Figure <A HREF="#fig1">2</A> is a diagram which shows how function calls work in RL-Glue.

<P>
The Experiment Program contains the ``main function''" which will then make all of it's requests for information through RL-Glue. These requests are usually related to setting up, starting and running the experiment and then gathering data about the agent's performance. The experiment program should never have access to the agent or environment directly, all contact should always go through the RL-Glue interface first.  There is also no direct contact between the agent and the environment. Any information the agent or environment returns is passed through RL-Glue to the module which needs it.  

<P>

<H2><A NAME="SECTION00031000000000000000">
Agents, Environments and Experiment Programs</A>
</H2>
Understanding agents and environments is a fundamental part of understanding reinforcement learning. A more detailed explanation of reinforcement learning, and its definitions for agents and environments, can be obtained from Reinforcement Learning: An Introduction (Sutton, Barto. 1998).  The agent, when distilled to basics, is both the learning algorithm and the decision maker. For RL-Glue's purpose, the agent only needs to decide which action to take at every step. The environment should store all the relevant details of the world of your experiment. This should include knowledge of the current state representation (the observation) as well as some method for determining state transitions and rewards. When writing code for an experiment, RL-Glue has been structured to place separation between the agent and environment. This division of the agent and environment both helps create modularized code and is theoretically desirable.

<P>
The experiment program is familiar to anyone who has written code to run an reinforcement learning experiment on their own. Akin to the typical main function in many reinforcement learning experiments, an RL-Glue experiment program is a control loop which runs the agent through the environment <B>x</B> number of times, perhaps doing <B>y</B> trials of these <B>x</B> episodes, all the while gathering data about how efficiently the agent has behaved or how quickly it has learned. RL-Glue provides several functions (Section <A HREF="#ref">6</A>) to assist in writing an experiment program.

<P>
When writing an agent , environment or experiment program for RL-Glue it is necessary to adhere to the interfaces (the RL-Glue Protocol) set out in Section <A HREF="#ref">6</A>. 

<P>

<H1><A NAME="SECTION00040000000000000000"></A>
<A NAME="env"></A>
<BR>
RL-Glue Environment Programs
</H1>
The environment represents everything outside the agents direct control. In a gridworld, the environment is the grid, including obstacles, rewards, start states, termination conditions and transition function. In a robotic task, the environment would also include the robot's body, as the robot as the robot does not have complete, deterministic control over its motors. The environment is basically everything that is not the agent.

<P>
In RL-Glue, the environment is defined by a set of parameterized functions that the RL-Glue interface queries on behalf of the experiment program. These functions define what the environment does before an experiment begins, at the beginning of an episode, on every remaining step of an episode and after the experiment is completed. The following sections describe the basic requirements of an RL-Glue environment, a complete list of all environment functions and specific details on how to implement the environment functions in C and C++.

<P>

<H2><A NAME="SECTION00041000000000000000"></A>
<A NAME="envp1"></A>
<BR>
Essential Components Of A RL-Glue Environment
</H2>

<P>
Every RL-Glue environment must define and initialize the action and observation types (rewards are always real-valued scalars) and define the env_start and env_step functions.   

<P>

<H3><A NAME="SECTION00041100000000000000">
Observation and Action Encoding</A>
</H3>
The representation of the observation and the action must be one of the following: 

<UL>
<LI>an integer
</LI>
<LI>a double
</LI>
<LI>a character
</LI>
<LI>array of integers
</LI>
<LI>array of doubles
</LI>
<LI>array of characters
</LI>
</UL>
Although this seems restrictive, in practice it should be easy to represent many observation and action types in this format. In a gridworld, for example, the action can be a single integer with values 0-3 (for N,S,E,W) and the observation can also be an integer for the agent's grid position (which is also the state of the environment, in this case). In Mountain Car, the actions are discrete (0-2) and the observation is the cars position and velocity (both real numbers). The action can be represented with and integer and the observation can be a two dimensional double array. 

<P>
Details on how to encode observation and action types in C and C++ is described in the ``RL-Glue Technical Documentation" included the project source.

<P>

<H3><A NAME="SECTION00041200000000000000">
Environment Start</A>
</H3>
Writing env_start is very simple. The function takes no input and simply returns an observation. The env_start function signifies the beginning of an episode; env_start chooses the initial state of the environment and returns the corresponding observation. For example, the following pseudo code selects a random start state for a grid world and returns the observation:
<PRE>
1. env_start -&gt; observation
2.      state = rand()*num_states
3.      set observation equal to state
4. return observation
</PRE>
The RL-Glue Technical Documentation describes how to implement line 3 in C/C++.

<P>

<H3><A NAME="SECTION00041300000000000000">
Environment Step</A>
</H3>
The final essential piece of an RL-Glue environment is the env_step function. The env_step function must take an action as input and produce a observation, reward and a termination flag as output. In most RL problems the  env_step function updates the internal state of the environment, tests for end of episode and returns the new observation of state and current reward. In other words, step function encodes the state transition and reward functions. Keeping with the grid world example, the following step function would be a valid env_step function:
<PRE>
1. env_step(action) -&gt; reward, observation, flag 
2.      newState = updateState(a, oldState)
3.      flag = isTerminal()
3.      reward = -1
4.      set observation equal to newState
5.      oldState = newState
6. return reward, observation, flag
</PRE>
Here we assume the existence of a state update function and an isTerminal function that checks if the current state is a terminal state.

<P>
So thats it. Just define the types of actions and observations and write two functions and you have a valid RL-Glue compatible environment. In later sections we will go over advanced environment functions and describe the syntactic details of actually coding an environment in C and C++. 

<P>

<H2><A NAME="SECTION00042000000000000000"></A>
<A NAME="envp2"></A>
<BR>
Additional Components Of A RL-Glue Environment
</H2>

<P>
So far we have only scratched the surface of what kinds of environments you can write in RL-Glue. Additional environment function can be written to initialize data structures, get and set the state of the environment, get and set the random seed and send generic string messages to the environment. Before we go on describing these functions it is useful to understand the task specification language that is used in RL-Glue to encode basic information about environments. 

<P>

<H3><A NAME="SECTION00042100000000000000"></A>
<A NAME="task"></A>
<BR>
Task Specification Language
</H3>
In an effort to provide the agent writer with simple and concise information about the environment a Task_specification is passed from the environment, through the interface, to the agent. The environment's init method (env_init) encodes information about the problem in a ASCII string. The string is then passed to the agent's init method (agent_init).  This information can also be used to check that the agent and environment are suitable for each other. A few example Task_specifications are provided below.

<BR>
<BR>
The agent is responsible for parsing any relevant information out of the Task_specification in the init method. A generic Task_specification parsing function is provided with RL-Glue 2.0 for all C/C++ users. This simple parser will return a structure containing the information encoded in the Task_specification, such as the Observation and Action dimensions, arrays of Observation and Action variable ranges, and arrays of Observation and Action variable types. More information about the parser and the parsed task spec struct can be found here.

<BR>
<BR>
The Task_specification is stored as a string with the following format:
<PRE>
     "V:E:O:A:R"
</PRE>		
For example, this is a sample task_specification provided as one of the examples below:
<PRE>
     "2:e:1_[i]_[0,N-1]:1_[i]_[0,3]:[-1,0]"
</PRE>     
The V corresponds to the version number of the task specification language. E corresponds to the type of task being solved. It has a character value of `e' if the task is episodic and `c' if the task is continuing. O and A correspond to Observation and Action information respectively. Finally, the R corresponds to the range of rewards for the task. Within each of O, A and R a range can be provided, however if the values are unknown or infinite in magnitude, two special input values have been defined.

<BR>
<BR>
The format of O and A are identical. We will describe the form of O only. O contains three components, separated by underscore characters ("_") :
<PRE>        
     #dimensions_dimensionTypes_dimensionRanges
</PRE>
#dimensions is an integer value specifying the number of dimensions in the Observation space. dimensionTypes is a list specifying the type of each dimension variable. The dimensionTypes list is composed of #dimensions components separated by comma characters (``,") within square brackets ([x1,x2,x3,..., xn] where xi  represents the ith value). Each comma-separated value in the list describes the type of values assigned to each Observation variable in the environment. In general, Observation variables can have one of the following 2 types:
<PRE>
     `i' - integer value
     `f' - float value
</PRE>
Thus a dimensionTypes list corresponding to an Observation space with 1 dimension has the following form:
<PRE>
     [a] where a is an element of [`i',`f']
</PRE>
So a dimensionTypes list with one integer value would be:    
<PRE>    
     [i]
</PRE>     
An Observation space with 2 dimensions would have a dimensionTypes with the following form:
<PRE>
     [a,b] where a and b are elements of [`i',`f']
</PRE>
indicating the value type of the first (a) and second (b) Observation variables. Thus a three dimensional Observation with one float, integer dimension and another float dimension would have the following dimensionTypes:
<PRE>
     "[f,i,f]"
</PRE>
The dimensionRanges is a list specifying the range of each dimension variable in the Observation space.  The dimensionRanges is composed of #dimensions components separated by underscore characters. Each dimensionRanges component specifies the upper and lower bound of values for each Observation variable. If the bounds are unknown or unspecified, you can leave an empty space in the place of a value. If the bounds are positive or negative infinity, you can use inf or -inf to represent your range. These can be used in combination. For example one valid range could be an unknown lower bound and infinite upper bound, or a lower bound of -inf and an upper bound of 1. You can be as precise (though you must be accurate) as you wish. A dimensionRanges corresponding to an Observation space with a single dimension variable would have the following form:
<PRE>
     [O1MIN, O1MAX]
</PRE>
So a dimensionRanges list for binary dimension varaible would be: [0,1]
A dimensionRanges list for variable with no upper or lower bound unspecified would be: [] or [,] (both are valid).
A dimensionRanges with one or two unbounded value can take on a value of inf or -inf. Eg [0, inf] or [-inf,1] or [-inf,inf].

<BR>
<BR>
An Observation space with 2 dimensions would have a dimensionRanges with the following form:
<PRE>
     [O1MIN, O1MAX]_[O2MIN, O2MAX]
</PRE>
indicating the minimal and maximal values of Observation variables O1 and O2 respectively. This definition can be then trivially extended to Observation spaces with N dimensions.

<BR>
<BR>
NOTE: the dimensionRanges of an Observation space with 1 or more unbounded values may not be representable in this way. An unbounded value has no minimal or maximal range. Thus, we simply do not specify the range in the dimensionRanges for any Observation variables with unbounded values. For example, consider a problem with 3 Observation dimensions. The first and third Observation variables have interval values and the second has unbounded ratio value. The corresponding dimensionRanges for this problem is encoded as:
<PRE>
     [O1MIN, O1MAX]_[,]_[O3MIN, O3MAX]
</PRE>
indicating the minimal and maximal values of Observation variables O1 and O3.

<BR>
<BR>
The format of A (Action space information) is identical to that of O (Observation space information) and thus the definitions above hold for Action spaces.

<BR>
<BR>
Lastly the R (Reward space information) is merely a range specifier. By the Reward Hypothesis there is only ever one reward signal (which in RL-Glue is always a floating point number) so the #dimensions and dimensionType information becomes meaningless. The reward range can again be specified to be unknown or infinite in the same manner as the Observation ranges.  A rewardRange follows the following form:
<PRE>
     [rewardMin, rewardMax]
</PRE>
In the case of a reward with rewards -1 or 0 the rewardRange would appear as such: [-1,0].
If no lower bound was known and the upper bound was positive infinity, the rewardRange would appear as such: [,-inf]

<H3><A NAME="SECTION00042200000000000000">
Example Task_Specification</A>
</H3>
Consider a simple gridworld with Actions North, South, East and West and a single dimension Observation of grid position. If we encode actions as 0, 1 ,2 ,3 and position as an integer between 0 and N-1, we get the following Task_specification:
<PRE>
     "2:e:1_[i]_[0,N-1]:1_[i]_[0,3]:[-1,0] "
</PRE>
This Task_specification provides the following information:

<UL>
<LI>Task Specification version 2.0 supported
</LI>
<LI>the task is episodic
</LI>
<LI>Observation space has one dimension
</LI>
<LI>the Observation variable has integer values (discrete state)
</LI>
<LI>range of Observation variable is 0 to N-1
</LI>
<LI>Action space has one dimension
</LI>
<LI>the Action variable has integer values (discrete actions: tabular)
</LI>
<LI>range of Action variable is 0 to 3
</LI>
<LI>range of the rewards is -1 to 0
</LI>
</UL>

<P>

<H3><A NAME="SECTION00042300000000000000">
Environment initialization and Cleanup</A>
</H3>        
Most environments need to store an internal state representation  and therefore many environment programs you write will need to allocate and deallocate data structures before and after a learning experiment. The env_init function allocates any global data structures and variables that will be accessed by the start and step functions. For example, the env_init method might initialize the tabular state variable to zero and allocate a <I>numStates X numStates</I> state transition array. The env_init function can optionally define a Task Specification string. The env_init function must return a string, but it can return NULL or an empty string if the Task Specification is not required for your experiment. The Task Specification is described in detail in the previous section (Section <A HREF="#task">3.2.1</A>).

<P>
The env_cleanup function usually deallocates or frees anything allocated in env_init.

<P>

<H3><A NAME="SECTION00042400000000000000">
Environment Message</A>
</H3>
Once in a while you will find yourself wishing there was an environment function that was part of RL-Glue. The env_message function allows you to exactly this: you can send a string message to the environment and it can respond with a string. For example to disable random starting states:
<PRE>
1. env_message(inMessage) -&gt; outMessage
2.      if inMessage == "turnOffRandomStarts"  
3.           randStarts = false
4.      end 
5. return "1"
</PRE>

<P>

<H3><A NAME="SECTION00042500000000000000">
Environment Get and Set Sate</A>
</H3>
It is often necessary to record the state of the environment and then reset the environment to a particular state to evaluate learning performance. For example, one might want to repeatedly evaluate the performance of a stochastic policy starting from a finite set of interesting states. The env_get_state_key function returns a state_key that can be used to restore the environment to the current state when the key was created. The state_key is the same data type as the observations and actions; the state_key must be an int, double, character, int array, double array or character array. The env_set_state takes a state_key as input and simply restores the environment to the state encoded in the key. The state_key can be as simple as a integer state label, in the case of a gridworld, or a some function of an array, in the case of a simulated robot navigation task. 

<P>
The get and set state functions are meant to be used during a single learning experiments not across runs. For example, the following experiment is valid:
<PRE>
1. initialize RL-Glue
2. until observation "cliff" do next step of episode
3. get state key form environment and store in stateKey
4. for 100 steps
5. set environment state to stateKey
6. run an episode
</PRE>
Storing a state key in a file and then using that state key in a different experiment program or later time is outside the intended usage of the env_get_state and env_set_state functions. It might work with some RL-Glue environments and not others.

<P>

<H3><A NAME="SECTION00042600000000000000">
Environment Get and Set Random Seed</A>
</H3>
It can be useful to record and set the seed of the random number generator. Although the random seed can be well thought of as part of the state of the environment there may be times when it is more convenient access the random seed only with env_get_random_seed and env_set_random_seed functions. The environment get and set random functions return and take as input a random_seed_key, like the state_key for setting states. 

<P>
Like the get and set state functions, env_get_random_seed and env_set_random_seed functions should not be used to store and set the random seed between distinct runs of RL-Glue.

<P>
A compete list of environment functions can be found in Section <A HREF="#Eref">6.2</A>.

<P>

<H1><A NAME="SECTION00050000000000000000"></A>
<A NAME="agent"></A>
<BR>
RL-Glue Agent Programs
</H1>
An RL-Glue agent can be as simple as a program the returns a random number between one and zero on every step or a more advanced algorithm that learns a model of the reward and transition functions of environment while maximizing reward. The agent program is a decision maker first and foremost: it must return an action when queried by RL-Glue. Many Rl-Glue agents, however, learn the best action to pick by learning from the sequence of observations, actions and rewards during an episode. Agent programs, like environments, are completely defined by a set of functions you must implement. Rl-Glue calls these agent functions during an experiment, as directed by the experiment program. Whether you are writing a random agent or a learning agent you usually need only implement a few functions. This section covers the basic requirements of an RL-Glue agent, describes a number of optional agent functions and describes how to implement these agent functions in C and C++.

<H2><A NAME="SECTION00051000000000000000"></A>
<A NAME="agentp1"></A>
<BR>
Essential Components Of A RL-Glue Agent
</H2>

<P>
An agent program is fully compatible with RL-Glue if it implements three functions: agent_start, agent_step and agent_end. 

<P>

<H3><A NAME="SECTION00051100000000000000">
Action Types</A>
</H3>
The three agent functions take observations and rewards as input and return actions. The observations and rewards are created by the environment, so the agent program needs to only read their values. The actions, however, must be defined by the agent.

<P>

<H3><A NAME="SECTION00051200000000000000">
Agent Start</A>
</H3>
The agent_start function selects the first action at the beginning of an episode based on the first observation of the environment. The agent_start function does not receive a reward as input; agent_start usually contains no learning update code. For example, the following function selects the first action based on the current value function estimate:
<PRE>
1. agent_start (observation) -&gt; action
2.      for each action i
3.           if highest valued action (Q(observation,i))
4.           then store i as maxAction
5.      set action equal to newAction
6. return action
</PRE>
The following sections will describe how to perform step 5 in several different programming languages.

<P>

<H3><A NAME="SECTION00051300000000000000">
Agent Step</A>
</H3>
The agent_step function encodes the heart of the agents' learning algorithm and action selection mechanism. At a minimum the step function must return an action every time it is called. In most learning agents, the step function queries the agent programs action selection function and performs a learning update based on the input observation and reward. The following agent_step function does a SARSA update on a tabular value function Q:
<PRE>
1. agent_step(reward, observation)-&gt; action
2.      newAction = egreedy(observation)
3.      QofOld = Q(oldState,oldAction)
4.      QofNew = Q(observation,newAction) 
5.      Q(oldState,oldAction) = QofOld + alpha*[reward + gamma*QofNew - QofOld]
6.      oldState = observation
7.     oldAction = newAction
8.      set action equal to newAction
9. return action
</PRE>  
Notice that the agent program must explicitly store the observation and action from the previous time step. RL-Glue does not make the history of actions, observations and rewards available to the agent or environment.

<P>

<H3><A NAME="SECTION00051400000000000000">
Agent End</A>
</H3>
In episode task the environment enters a terminal state that ends the episode. RL-Glue responds to the end of an episode by calling the agent_end function; passing the reward produced on the last transition to the agent and signaling the end of the current episode. The agent_end function usually performs a final learning update based on the last transition and any other end-of-episode routines, such as clearing eligibility traces. If the environment is non-episodic RL-Glue will not query agent_end.

<P>
The agent_end function does not receive the final observation from the environment. In many learning problems this is of no consequence because the agent does not make a decision in the terminal state. If, however, the agent were learning a model of the environment, information about the final transition would be important. In this case, it is recommended that the environment be augmented with a terminal state that has a reward of zero on the transition into it. This choice was made to keep the RL-Glue interface as minimal and light-weight as possible. 

<P>

<H2><A NAME="SECTION00052000000000000000"></A>
<A NAME="agentp2"></A>
<BR>
Additional Components Of A RL-Glue Agent
</H2>

<P>

<H3><A NAME="SECTION00052100000000000000">
Agent Initialization and Cleanup</A>
</H3>
Agent programs, like environments, often need to allocate and free various data structures. The agent_init and agent_cleanup function do not take any input or return an data and are called at the beginning and end of a learning experiment, respectively. The agent_init function also receives the Task Specification string as input. The agent_init function usually parses the Task Specification and stores various information encoded in the string. For example, after parsing the Task Specification, the agent_init function can then initialize the value function array to the size state space using the number of states from the Task Specification. Remember the Task Specification is not required to contain any information (NULL or empty string). The RL-Glue codecs provide parsers that return a structure/object given a Task Specification string.     

<P>

<H3><A NAME="SECTION00052200000000000000">
Agent Message</A>
</H3>
The agent_message function isused to send an arbitrary string message to the agent program. This function can be used to change agent parameters, notify the agent that the exploration phase is over, and request the name of the agent program, for example.

<P>

<H1><A NAME="SECTION00060000000000000000"></A>
<A NAME="exp"></A>
<BR>
RL-Glue Experiment Programs
</H1>
Usually the shortest and easiest part of writing your first learning experiment is writing the experiment program. The experiment program has no interface to implement and is mostly comprised of calls to the already existing RL-Glue functions. The experiment program has four main duties: a) start the experiment b) specify how many times to run the experiment c) extract data and possibly analyze d) end the experiment and clean up.  One thing to note is that it is only the RL-Glue interface functions available to the experiment program. No agent or environment implemented functions should be directly accessed by the experiment program.

<P>

<H2><A NAME="SECTION00061000000000000000"></A>
<A NAME="expp1"></A>
<BR>
Basic Experiment Programs
</H2>

<P>
At a minimum the experiment program must call RL_init and RL_cleanup and execute several time steps of agent-environment interaction. The following pseudo code represents a simple experiment program.
<PRE>
1. RL_init()
2. RL_start()
3. steps=0
4. terminal=false 
5. while steps &lt; 100 and not terminal
6.     terminal,reward,observation,action = RL_step()
7. RL_cleanup()
</PRE>
This experiment program initializes the agent and environment (RL_init), calls the start functions of the agent and environment (RL_start) and then executes a 100 or less step episode. 

<P>
The RL_step function calls the env_step function passing it the most recent agent action (in this case from agent_start). The env_step function returns the new observation, reward and terminal flag. If the flag <B>is not</B> set the agent_step function is called with the new observation and reward as input arguments. The action returned by agent_step is stored by RL-Glue until the next call to RL_step. If the flag <B>is</B> set, the agent_end function is called with the reward as input. This process continues until either the flag is set or 100 steps are completed. 

<P>
Using the RL_step function gives the experiment program designer access to all the data produced during an episode; however, it is often more convient to use the RL_episode function when step-level control is not needed. Lines 5 and 6, in the above experiment program, can be replaced by a single call to RL_episode(100). If the input to RL_episode is zero, control will return to the experiment program if and only if the environment enters a terminal state.

<P>
The RL_step function allows the experiment program to record/sum/average the reward, but the RL_episode function returns no values. The RL_return and RL_num_steps functions allow the experiment program to obtain reward and number of steps taken. Specifically, RL_return returns the sum of rewards accumulated during the current or most recently completed episode. The RL_num_steps returns the number of steps elapsed during the current or most recently completed episode. 

<P>
Putting these new functions together we can write a more useful experiment program:
<PRE>
1. RL_init()
2. theReturn = 0
3. for 1 = 1:100
4.      RL_episode(1000)
5.      theReturn += RL_return()
6.  Print theReturn/100
7. RL_cleanup()
</PRE>
The above experiment program runs 100 episodes, each with max length 1000, and computes the average cumulative reward per episode.

<H2><A NAME="SECTION00062000000000000000"></A>
<A NAME="expp2"></A>
<BR>
Advanced Experiment Programs
</H2>

<P>
As you know from previous sections (about agent and environment programs) there are several optional agent and environment functions that provide more advanced control. These functions are access through calls to the RL-Glue interface from an experiment program. For example, to send a message to the agent use RL_agent_message. To get the state key from the environment use RL_get_state. The pattern is simple to follow:
<PRE>
     RL_set_state() -&gt; env_set_state()
     RL_get_random_seed() -&gt; env_get_random_seed()
     RL_set_random_seed() -&gt; env_set_random_seed()
     RL_env_message() -&gt; env_message
</PRE>
We can now produce more advanced experiment programs that would be used in reinforcement learning research:
<PRE>
1. RL_init()
2. numSteps = 0
3. for 1 = 1:1000
4.      RL_episode(1000)
5. RL_agent_message("freezeAgentPolicy")
6. for 1 = 1:100
7.      RL_episode(1000)
8.      numSteps += RL_num_steps()
9.  Print numSteps/100
10. RL_cleanup()
</PRE>

<P>

<H1><A NAME="SECTION00070000000000000000"></A>
<A NAME="ref"></A>
<BR>
Command and Function Reference
</H1>
Once your comfortable with the basics and you have written a few agents and environment you may often find yourself wondering ``I need to do <B>X</B> what function should I use?''. You may also wonder what is the ``intended purpose'' of a particular RL-Glue function, when you are reviewing someone else's agent or environment code. This section provides a complete listing of all agent, environment and RL-Glue interface function for quick reference.

<H2><A NAME="SECTION00071000000000000000">
Agent Functions</A>
</H2>

<P>
Every agent must define all of the following routines. Note these functions are only accessed by the RL-Glue. Experiment programs should not try to bypass the Glue and directly access these functions.
<PRE>
     agent_start(first_observation) -&gt; first_action
</PRE>
Given the first_observation (the observation of the agent in the start state) the agent must then return the action it wishes to perform. This is called once if the task is continuing, else it happens at the beginning of each episode.
<PRE>
     agent_step( reward, observation) -&gt; action
</PRE>
This is the most important function of the agent. Given the reward garnered by the agent's previous action, and the resulting observation, choose the next action to take. Any learning (policy improvement) should be done through this function.
<PRE>
     agent_end(reward)
</PRE>     
If the agent is in an episodic environment, this function will be called after the terminal state is entered. This allows for any final learning updates. If the episode is terminated prematurely (ie a benchmark cutoff before entering a terminal state) agent_end is NOT called.
<PRE>
     agent_init(task_specification)
</PRE>     
This function will be called first, even before agent_start. The task_specification is a description of important experiment information, including but not exclusive to a description of the state and action space. The RL-Glue standard for writing task_specification strings is found here.  In agent_init, information about the environment is extracted from the task_specification and then used to set up any necessary resources (for example, initialize the value function to a prelearning state).
<PRE>
     agent_cleanup()
</PRE>     
This function is called at the end of a run/trial and can be used to free any resources which may have allocated in agent_init. Calls to agent_cleanup should be in a one to one ratio with the calls to agent_init.
<PRE>
     agent_freeze()
</PRE>
Signals to the agent that training has ended. Requests that the agent freeze its current policy and value function (ie: stops learning and exploration).
<PRE>
     agent_message(input_message) -&gt; output_message
</PRE>     
The agent_message function is a jack of all trades and master of none. Having no particular functionality, it is up to the user to determine what agent_message should implement. If there is any information which needs to be passed in or out of the agent, this message should do it. For example, if it is desirable that an agent's learning parameters be tweaked mid experiment, the author could establish an input string that triggers this action. Likewise, if the author wished to extract a representation of the value function, they could establish an input string which would cause agent_message to return the desired information.

<P>

<H2><A NAME="SECTION00072000000000000000"></A>
<A NAME="Eref"></A>
<BR>
Environment Functions
</H2>
Every environment must define all of the following routines. Note these functions are only accessed by the RL-Glue. Experiment programs should not try to bypass the Glue and directly access these functions.
<PRE>
     env_start() -&gt; first_observation
</PRE>
For a continuing task this is done once. For an episodic task, this is done at the beginning of each episode. Env_start assembles a first_observation given the agent is in the start state. Note the start state cannot also be a terminal state.
<PRE>
     env_step(action) -&gt; reward, observation, terminal
</PRE>
Complete one step in the environment. Take the action passed in and determine what the reward and next state are for that transition.
<PRE>
     env_init() -&gt; task_specification
</PRE>
This routine will be called exactly once for each trial/run. This function is an ideal place to initialize all environment information and allocate any resources required to represent the environment. It must return a task_specification which adheres to the task specification language. A task_specification stores information regarding the observation and action space, as well as whether the task is episodic or continuous.
<PRE>
     env_get_state() -&gt; state_key
</PRE>
The state_key is a compact representation of the current state of the environment such that at any point in the future, provided with the state_key, the environment could return to thatstate. Note that this does not include the agent's value function, it is merely restoring the details of the environment. For example, in a static grid world this would be as simple as the position of the agent.
<PRE>
     env_set_state(state_key)
</PRE>
Given the state_key, the environment should return to it's exact formation when the state_key was obtained. 
<PRE>
     env_get_random_seed() -&gt; random_seed_key
</PRE>
Saves the random seed object used by the environment such that it can be restored upon presentation of random_seed_key.
<PRE>
     env_set_random_seed(random_seed_key)
</PRE>
Sets the random seed used by the environment. Typically it is advantageous for the experiment program to control the randomness of the environment. Env_set_random_seed can be used in conjunction with env_set state to save and restore a random_seed such that the environment will behave exactly the same way it has previously when it was in this state and given the same actions.
<PRE>                 
     env_cleanup()
</PRE>
This can be used to release any allocated resources. It will be called once for every call to env_init.
<PRE>
     env_message(input_string) -&gt; output_string
</PRE>
Similar to agent_message, this function allows for any message passing to the environment required by the experiment program. This may be used to modify the environment mid experiment. Any information that needs to passed in or out of the environment can be handled by this function.

<P>

<H2><A NAME="SECTION00073000000000000000">
Interface Routines Provided by the RL-Glue</A>
</H2>

<P>
The following built in RL-Glue functions are provided primarily for the use of the experiment program writers. Using these functions, the experiment program gains access to the corresponding environment and agent functions. The implementation of these routines are to be standard across all RL-Glue users. To ensure agents/environments/experiment programs can be exchanged between authors with no changes necessary, users should not change the RL-Glue interface code provided.

<BR>
<BR>
To understand the following, it is helpful to think of an episode as consisting of sequences of observations, actions, and rewards that are indexed by time-step as follows:
<PRE>
     o0, a0,  r1, o1, a1,  r2, o2, a2, ..., rT, terminal_observation
</PRE>
where the episode lasts T time steps (T may be infinite) and terminal_observation is a special, designated observation signaling the end of the episode.

<P>
<PRE>
RL_init()
     agent_init(env_init())
</PRE>
This initializes everything, passing the environment's task_specification to the agent. This should be called at the beginning of every trial.
<PRE>
RL_start() --&gt; o0, a0
     global upcoming_action
     o = env_start()
     a = agent_start(o)
     upcoming_action = a
return o,a
</PRE>
Do the first step of a run or episode.  The action is saved in upcoming_action so that it can be used on the next step.
<PRE>
RL_step() --&gt; rt, ot, terminal, at
     global upcoming_action
     r,o,terminal = env_step(upcoming_action)
     if terminal == true
           agent_end(r)
           return r, o,terminal
     else
           a = agent_step(r, o)
           upcoming_action = a
     return r, o, terminal, a
</PRE>
Take one step.  RL_step uses the saved action and saves the returned action for the next step.  The action returned from one call must be used in the next, so it is better to handle this implicitly so that the user doesn't have to keep track of the action.  If the end-of-episode observation occurs, then no action is returned.
<PRE>
RL_episode(steps)
     num_steps = 0
     o, a = RL_start()
     num_steps = num_steps + 1
     list = [o, a]
     while o != terminal_observation
           if(steps !=0 and num_steps &gt;= steps)
                end
           else
                r, o, a = RL_step()
                list = list + [r, o, a]
                num_steps = num_steps + 1
           
     agent_end(r)
</PRE>
Do one episode until a termination observation occurs or until steps steps have elapsed, whichever comes first.  As you might imagine, this is done by calling RL_start, then RL_step until the terminal observation occurs.  If steps is set to 0, it is taken to be the case where there is no limitation on the number of steps taken and RL_episode will continue until a termination observation occurs. If no terminal observation is reached before num_steps is reached, the agent does not call agent_end, it simply stops.
<PRE>
     RL_return() -&gt; return
</PRE>
Return the cumulative total reward of the current or just completed episode.  The collection of all the rewards received in an episode (the return) is done within RL_return however, any discounting of rewards must be done inside the environment or agent.
<PRE>
     RL_num_steps() -&gt; num_steps
</PRE>
Return the number of steps elapsed in the current or just completed episode.
<PRE>
     RL_cleanup()
          env_cleanup()
          agent_cleanup()
</PRE>
Provides an opportunity to reclaim resources allocated by RL_init.
<PRE>
     RL_set_state(State_key)
          env_set_state(State_key)
</PRE>
Provides an opportunity to reset the state (see env_set_state for details).
<PRE>
     RL_set_random_seed(Random_seed_key)
          env_set_random_seed(Random_seed_key)
</PRE>
Provides an opportunity to reset the random seed key (see env_set_random_seed for details).
<PRE>
     RL_get_state() -&gt; State_key
          return env_get_state()
</PRE>
Provides an opportunity to extract the state key from the environment (see env_get_state for details).
<PRE>
     RL_get_random_seed() -&gt; Random_seed_key
          return env_get_random_seed()
</PRE>
Provides an opportunity to extract the random seed key from the environment (see env_get_random_seed for details).               
<PRE>
     RL_agent_message(input_message_string) -&gt; output_message_string
          return agent_message(input_message_string)
</PRE>
This message passes the input string to the agent and returns the reply string given by the agent. See agent_message for more details.                
<PRE>
     RL_env_message(input_message_string) -&gt; output_message_string
          return env_message(input_message_string)
</PRE>
This message passes the input string to the environment and returns the reply string given by the environment. See env_message for more details. 

<P>

<H1><A NAME="SECTION00080000000000000000"></A>
<A NAME="change"></A>
<BR>
Changes from RL-Glue 2.x
</H1>
Version 3.0 of RL-Glue represents a large series of updates with the intention of bringing RL-Glue from a University of Alberta side project to an open source project generally useful to the global
reinforcement learning community.  As part of that process, we made some pretty big changes.

<P>

<H2><A NAME="SECTION00081000000000000000">
The Codec Split</A>
</H2>
The codecs (including the C/C++ codec) has been split from the main RL-Glue project.  Each codec package is now more independently, and they each may offer something different to the user.  However, as always, they layer on RL-Glue.  The root page for all of the codecs is:
<BR>
<A NAME="tex2html3"
  HREF="http://glue.rl-community.org/Home/codecs">http://glue.rl-community.org/Home/codecs</A>
<P>
As usual, you don't need to change your code depending on how it will be used.  The code for an agent, environment, or experiment is identical no matter if you will run it using sockets or directly compiled together.  The only difference is what library you link against.

<P>

<H3><A NAME="SECTION00081100000000000000">
RL-Glue Project</A>
</H3>
The idea of the RL-Glue project is that it will very very rarely change.  We've made most of the changes on our wishlist, so RL-Glue can become a library that is standard and reliable for doing reinforcement learning experiments.  The goal is that it is always there, and it always just works.

<P>
This project is written entirely in C and can be linked from C or C++ code.

<P>
When you download and install the RL-Glue project, you get three artifacts:
<DL>
<DT><STRONG>rl_glue executable socket server</STRONG></DT>
<DD>The server for running socket-based experiments.
	
</DD>
<DT><STRONG>librlglue</STRONG></DT>
<DD>A C library that can be linked against for creating executables where the agent, environment, and experiment program are all written in C or C++. These experiments have virtually no rl-glue calling overhead.
	
</DD>
<DT><STRONG>Agent/Environment/Experiment Headers</STRONG></DT>
<DD>Four header files: RL_glue.h Agent_common.h, Environment_common.h, and RL_common.h. 
</DD>
</DL>

<P>
The way that they should be included in your agents/environments/experiments is like this:
<PRE>
&lt;rlglue/RL_common.h&gt;             /* Datastructures                                     */
&lt;rlglue/RL_glue.h&gt;               /* Experiment (RL_) functions for experiments 
                                  (includes RL_common)                                 */
&lt;rlglue/Agent_common.h&gt;          /* Agent (agent_) functions (includes RL_common)      */
&lt;rlglue/Environment_common.h&gt;    /* Environment (env_) functions (includes RL_common)  */
&lt;rlglue/Environment_common.h&gt;    /* Environment (env_) functions (includes RL_common)  */
&lt;rlglue/utils/C/RLStruct_util.h&gt; /* Handy utility functions for copying/initing structs*/
</PRE> 

<P>
Generally, each of agent/env/experiment should only have to include one of these files.  You'll probably never include RL_common.h, but it is needed by the others.

<P>

<H3><A NAME="SECTION00081200000000000000">
RL-Glue-Extensions Project :: C/C++ Codec</A>
</H3>
The <A NAME="tex2html4"
  HREF="http://glue.rl-community.org/Home/codecs/c-c-codec">C/C++ codec</A>
gives you libraries that can be used to build stand-alone socket-based agents, environment, and experiments.  The C Codec is in the rl-glue-ext project and is expected to change more frequently than the main RL-Glue project.

<P>
This project is written entirely in C and can be linked from C or C++ code.

<P>
The artifacts of the C Codec are:
<DL>
<DT><STRONG>librlagent</STRONG></DT>
<DD>Library give agents what they need to connect to the rl_glue executable server over sockets.
	
</DD>
<DT><STRONG>librlenvironment</STRONG></DT>
<DD>Library give environments what they need to connect to the rl_glue executable server over sockets.
	
</DD>
<DT><STRONG>librlexperiment</STRONG></DT>
<DD>Library give experiments what they need to connect to the rl_glue executable server over sockets.
</DD>
</DL>

<P>

<H2><A NAME="SECTION00082000000000000000">
Build Changes</A>
</H2>
We're not manually writing Makefiles anymore!  We've moved both RL-Glue and the C/C++ Codec to a <A NAME="tex2html5"
  HREF="http://www.gnu.org/software/autoconf/">GNU autotools system</A>. You can build these projects using the following standard Linux/Unix procedure now:
<PRE>
	&gt;$ ./configure
	&gt;$ make
	&gt;$ sudo make install
</PRE>

<P>

<H2><A NAME="SECTION00083000000000000000">
API Changes</A>
</H2>
All of the API changes have been included in all of the codecs.

<P>

<H3><A NAME="SECTION00083100000000000000">
RL_Freeze and Agent_Freeze</A>
</H3>
The freeze methods were the first in a long series of ``special'' methods that some people wanted RL-Glue to support.  The long term solution to special methods is the messaging system, RL_agent_message and RL_env_message. With the messaging system you can create any protocol you want between your experiment and agent or experiment and environment.

<P>
So, to reduce the clutter and kruft (cruft?) of the API, we've removed Freeze.  How do you unfreeze anyways?

<P>

<H3><A NAME="SECTION00083200000000000000">
RL_Episode</A>
</H3>
Csaba Szepesvri made the request at some point that <TT>RL_Episode</TT> should let you know whether it ended because the time step limit expired, or because the episode ended normally.  We now return the value of the terminal flag from the last <TT>env_step</TT> of the episode. If the flag is set to 1, then the episode terminated normally, if not, the timeout ended the episode.  

<P>

<H3><A NAME="SECTION00083300000000000000">
RL_Init</A>
</H3>
It made sense to us that <TT>RL_init</TT> should return the task spec, in case the experiment program wants to know it, make a note of it, etc. The RL_glue specification has now been updated to handle this.

<P>

<H2><A NAME="SECTION00084000000000000000">
Type Changes</A>
</H2>
All of the Type changes have been included in all of the codecs.

<P>

<H3><A NAME="SECTION00084100000000000000">
Typedefs</A>
</H3>
This is a big one. We revamped all of the type names for C/C++.  We made them all lower case, and added ``_t'' to them to identify them as types.  This should reduce confusion so there is no more code like:
<BR>
<TT>Observation observation;</TT>

<P>
Instead it'll be:
<BR>
<TT>observation_t observation;</TT>

<P>
We think the latter is easier to read.

<P>
This is a pain to change if you have a lot of existing code, so we've made it easy to transition.  There is a file you can include which will define all the old, ugly type names. This is good news because it means you can migrate to the new type names at your leisure.
<PRE>
#include &lt;rlglue/legacy_types.h&gt;
</PRE>

<P>
You can find all the old and new type names here:
<BR>
<A NAME="tex2html6"
  HREF="http://code.google.com/p/rl-glue/source/browse/trunk/src/rlglue/legacy_types.h">http://code.google.com/p/rl-glue/source/browse/trunk/src/rlglue/legacy_types.h</A>
<P>

<H3><A NAME="SECTION00084200000000000000">
charArray!</A>
</H3>
Some people have found the interface of abstract types that are arrays of <TT>double</TT> and <TT>int</TT> a little bit too constricting.  We've added a third array, this time of <TT>char</TT>.  Now people can push strings and char arrays of anything they want through observations, action, state_key types, etc.

<P>
The rl_abstract_type_t now looks like:
<PRE>
typedef struct
{
    unsigned int numInts;
    unsigned int numDoubles;
    unsigned int numChars;
    int* intArray;
    double* doubleArray;
    char* charArray;
} rl_abstract_type_t;
</PRE>

<P>
Keep in mind that charArray is an array of characters.  It is not necessarily null terminated.  We don't enforce null termination. Remember, 3 chars takes up 3 array spots, but ``123'' takes up 4 (`
<BR>
0' at the end).

<P>
If you do the following, bad things will probably happen if the char array is not null terminated:
<PRE>
	printf("My char array is %s\n",observation.charArray);
</PRE>

<P>

<H1><A NAME="SECTION00090000000000000000"></A>
<A NAME="faq"></A>
<BR>
Frequently Asked Questions
</H1>

<P>

<H2><A NAME="SECTION00091000000000000000">
Where did the task spec parsers go?</A>
</H2>
The task specification language has always been a bit of a sensitive topic, and writing good parsers for the task spec has never been easy. We're currently considering slightly changing the task spec format, to make
it easier to read and easier to parse.  After that, we'll need (<B>Chance to Contribute!</B>) a parser for each codec.  We won't need to change the main RL-Glue project, but we will need to re-release all of the 
codec software.  So, we're waiting on deciding on changes to the language and implementing the parsers.

<P>

<H2><A NAME="SECTION00092000000000000000">
Can I write my agents in &lt; insert language here &gt;</A>
</H2>
Yes! Maybe.  Writing agents/environments/experiments in different languages require there to be a codec for that language.  As of writing, there are codecs for C/C++, Java, Matlab, Python, and Lisp.  Check out the codecs project for more information:
<BR>
<A NAME="tex2html7"
  HREF="http://glue.rl-community.org/Home/codecs">http://glue.rl-community.org/Home/codecs</A>
<P>

<H2><A NAME="SECTION00093000000000000000">
Does Rl-Glue support multi-agent reinforcement learning?</A>
</H2>
 No. RL-Glue is designed for single agent reinforcement learning. At present we are not planning a multi-agent extension of RL-Glue. We envision that this would be a separate project with a different audience and different objectives.  

<P>

<H2><A NAME="SECTION00094000000000000000">
Why isn't the RL-Glue interface object oriented?</A>
</H2>
RL-Glue is meant to be a low level protocol for connecting agents, environments, and experiments.  These interactions can
easily be described by the simple, flat, functions calls of RL-Glue.  We don't feel that it is useful to overcomplicate
things in that respect.

<P>
However, there is no reason that an implementation of an agent or environment shouldn't be designed using an object-oriented 
approach.  In fact, many of the contributors to this project have their own object-oriented libraries of agents that 
they use with RL-Glue.

<P>
Some might argue that it makes sense to create a C/C++ or Java codec that supports an OO design
directly.  This would not be hard, it's just a matter of someone interested picking up the project and doing it.  Personally, 
we've found it easy enough to write a small bridge between the existing codecs and our personal OO hierarchies.

<P>

<H1><A NAME="SECTION000100000000000000000">
Authors</A>
</H1>
A great many people have contributed to this project over the years.

<P>
<DL>
<DT><STRONG>Richard Sutton</STRONG></DT>
<DD>forever-long-ago to 2008
<BR>
Conceived of the project and developed it under many different names over the years.
<BR>
rich@richsutton.com

<P>
</DD>
<DT><STRONG>Adam White</STRONG></DT>
<DD>while graduate student at the University of Alberta, 2005-2008
<BR>
Brought RL-Glue together as his Masters Thesis topic.
<BR>
awhite@cs.ualberta.ca

<P>
</DD>
<DT><STRONG>Mark Lee</STRONG></DT>
<DD>while programmer at the University of Alberta, 2005-2008.
<BR>
Various contributions to the glue and python codecs.

<P>
</DD>
<DT><STRONG>Andrew Butcher</STRONG></DT>
<DD>while programmer at the University of Alberta, 2006-2007.
<BR>
Overhauled Adam White's code and created first C/C++ and Java codecs.
<BR>
andrew.butcher@uleth.ca

<P>
</DD>
<DT><STRONG>Brian Tanner</STRONG></DT>
<DD>while Ph.D student at the University of Alberta, 2006 - 2008.
<BR>
Various design and code contributions to RL-Glue, ported to GNU autotools build system.
<BR>
brian@tannerpages.com

<P>
</DD>
<DT><STRONG>Leah Hackman</STRONG></DT>
<DD>summer NSERC student at the University of Alberta, 2007.
<BR>
Various contributions to RL-Glue.

<P>
</DD>
<DT><STRONG>Matthew Radkie</STRONG></DT>
<DD>summer internship student at the University of Alberta, 2007-2008.
<BR>
Various contributions to RL-Glue while working on the RL-Competition project.
</DD>
</DL>
Last Updated:  Sept 2008

<H1><A NAME="SECTION000110000000000000000">
Document Information</A>
</H1>
<PRE>
Revision Number: $Rev: 864 $
Last Updated By: $Author: brian@tannerpages.com $
Last Updated   : $Date: 2008-09-30 20:50:49 -0600 (Tue, 30 Sep 2008) $
$URL: https://rl-glue.googlecode.com/svn/trunk/docs/TechnicalManual.tex $
</PRE>
<H1><A NAME="SECTION000120000000000000000">
About this document ...</A>
</H1>
 <STRONG>RL-Glue 3.0 Overview</STRONG><P>
This document was generated using the
<A HREF="http://www.latex2html.org/"><STRONG>LaTeX</STRONG>2<tt>HTML</tt></A> translator Version 2002-2-1 (1.71)
<P>
Copyright &#169; 1993, 1994, 1995, 1996,
<A HREF="http://cbl.leeds.ac.uk/nikos/personal.html">Nikos Drakos</A>, 
Computer Based Learning Unit, University of Leeds.
<BR>
Copyright &#169; 1997, 1998, 1999,
<A HREF="http://www.maths.mq.edu.au/~ross/">Ross Moore</A>, 
Mathematics Department, Macquarie University, Sydney.
<P>
The command line arguments were: <BR>
 <STRONG>latex2html</STRONG> <TT>Glue-Overview.tex -split 0 -dir html -mkdir -title 'RL-Glue 3.0' -local_icons -math</TT>
<P>
The translation was initiated by Brian Tanner on 2008-10-01<HR>
<!--Navigation Panel-->
<IMG WIDTH="81" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="next_inactive" SRC="nx_grp_g.png"> 
<IMG WIDTH="26" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="up" SRC="up_g.png"> 
<IMG WIDTH="63" HEIGHT="24" ALIGN="BOTTOM" BORDER="0" ALT="previous" SRC="prev_g.png">   
<BR>
<!--End of Navigation Panel-->
<ADDRESS>
Brian Tanner
2008-10-01
</ADDRESS>
</BODY>
</HTML>
